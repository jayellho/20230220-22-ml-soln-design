{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"MLSD-04-FeatureEngineering-A.ipynb\" target=\"_self\">Feature Engineering A</a> | <a href=\"./\">Content Page</a> | <a href=\"MLSD-04-FeatureEngineering-Ex-1.ipynb\">Feature Engineering Exercise</a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>FEATURE ENGINEERING B</center>\n",
    "\n",
    "<center><b>Copyright &copy 2023 by DR DANNY POO</b><br> e:dannypoo@nus.edu.sg<br> w:drdannypoo.com</center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "- Categorical features.\n",
    "- Text features.\n",
    "- Imputation of missing data.\n",
    "- Feature pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "data = [\n",
    "    {'price': 850000, 'rooms': 4, 'neighborhood': 'Changi'},\n",
    "    {'price': 720000, 'rooms': 3, 'neighborhood': 'Kembangan'},\n",
    "    {'price': 650000, 'rooms': 3, 'neighborhood': 'Pasir Panjang'},\n",
    "    {'price': 950000, 'rooms': 4, 'neighborhood': 'Woodlands'},\n",
    "    {'price': 830000, 'rooms': 4, 'neighborhood': 'Jurong'},\n",
    "    {'price': 680000, 'rooms': 2, 'neighborhood': 'Marine Parade'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- It would be wrong to encode as {'Changi': 1, 'Kembangan': 2, 'Pasir Panjang': 3, 'Woodlands': 4, 'Jurong': 5, 'Marine Parade': 6};\n",
    "- Instead use one-hot encoding which will create extra columns indicating the presence or absence of a category with a value of 1 or 0 respectively.\n",
    "- When data comes as a list of dictionaries (as above), can use Scikit-Learn's DictVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode using DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer(sparse=False, dtype=int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- The 'neighborhood' column has been expanded into 6 separate columns, representing the 6 neighborhood labels, and that each row has a 1 in the column associated with its neighborhood.\n",
    "- We can now fit the Scikit-Learn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the feature names\n",
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Disadvantage: if your category has many possible values, this can greatly increase the size of your dataset. \n",
    "- Since encoded data contains mostly zeros, use the spare output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set spare=True to reduce the size of matrix\n",
    "vec = DictVectorizer(sparse=True, dtype=int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- <b>sklearn.preprocessing.OneHotEncoder</b> and <b>sklearn.feature_extraction.FeatureHasher</b> are two additional tools that Scikit-Learn includes to support this type of encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "data = ['The Dark Knight',\n",
    "        'Batman and Robin',\n",
    "        'Man of Steel',\n",
    "        'Superman',\n",
    "        'Rise of the Dark Knight',\n",
    "        'Monty Python',\n",
    "        'The incredibles',\n",
    "        'The Golden Goose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a column representing each of the words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(data)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display data in a dataframe\n",
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Issue: The raw word counts lead to features which put too much weight on words that appear very frequently, and this can be sub-optimal in some classification algorithms. \n",
    "- Solution: Use <b>term frequency-inverse document frequency (TF–IDF)</b> which weights the word counts by a measure of how often they appear in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Applying term frequency-inverse document frequency (TF–IDF)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(data)\n",
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation of Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "X = np.array([[ nan, 0,   3  ],\n",
    "              [ 3,   7,   9  ],\n",
    "              [ 3,   5,   2  ],\n",
    "              [ 4,   nan, 6  ],\n",
    "              [ 8,   8,   1  ]])\n",
    "y = np.array([14, 16, -1,  8, -5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Need to first replace such missing data with some appropriate fill value. \n",
    "- This is known as imputation of missing values, and strategies range from simple (e.g., replacing missing values with the <b>mean of the column</b>) to sophisticated (e.g., using matrix completion or a robust model to handle such data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use Scikit-Learn SimpleImputer class\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputa = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "imputa.fit(X[:, 0:3]) # identifies the missing values and computes the mean of such feature where a missing value is present.\n",
    "\n",
    "# Replace the missing value using transform method\n",
    "X1 = imputa.transform(X[:, 0:3])\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- The two missing values have been replaced with the mean of the remaining values in the column. \n",
    "- This imputed data can then be fed directly into, for example, a LinearRegression estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X1, y)\n",
    "model.predict(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Pipelines\n",
    "- When there are multiple steps involved, can use feature processing pipeline:\n",
    "1. Impute missing values using the mean\n",
    "2. Fit a linear regression\n",
    "\n",
    "- Can use Scikit-Learn Pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Scikit-Learn Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(SimpleImputer(strategy='mean'),\n",
    "                      LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit a model\n",
    "model.fit(X, y)  # X with missing values, from above\n",
    "print(y)\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- All the steps of the model are applied automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"MLSD-04-FeatureEngineering-A.ipynb\" target=\"_self\">Feature Engineering A</a> | <a href=\"./\">Content Page</a> | <a href=\"MLSD-04-FeatureEngineering-Ex-1.ipynb\">Feature Engineering Exercise</a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"MLSD-02-DataPreprocessing-B.ipynb\" target=\"_self\">Data Preprocessing B</a> | <a href=\"./\">Content Page</a> | <a href=\"MLSD-02-DataPreprocessing-D.ipynb\">Data Preprocessing D | <a href=\"MLSD-02-DataPreprocessing-Ex-1.ipynb\">Data Preprocessing Exercise</a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>DATA PREPROCESSING C</center>\n",
    "\n",
    "<center><b>Copyright &copy 2023 by DR DANNY POO</b><br> e:dannypoo@nus.edu.sg<br> w:drdannypoo.com</center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<b>Dataset</b>: Online Shopper data set.<br>\n",
    "<b>Tasks</b>: \n",
    "- To read in and explore data set.\n",
    "- To fill missing data with mean values. \n",
    "- To encode categorical data.\n",
    "- To split dataset into training and test sets.\n",
    "- To rescale features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and Explore Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0       No \n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0       No \n",
       "3    Spain  38.0  61000.0       No \n",
       "4  Germany  40.0      NaN       Yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data set link\n",
    "path = \"./data/onlineShopper/dataset.csv\"\n",
    "\n",
    "# Prepare dataframe using the data at given link and defined columns list\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Array of features\n",
    "X = df.iloc[:, :-1].values # All rows and columns except the last column\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No ' 'Yes' 'No ' 'No ' 'Yes' 'Yes' 'No ' 'Yes' 'No ' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# Array of dependent variable\n",
    "y = df.iloc[:, -1].values # All rows of last column\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Missing Data with Mean Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Import class SimpleImputer from impute model in sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create object of SimpleImputer class\n",
    "imputa = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "''' Using the fit method, we apply the `imputa` object on the matrix of our feature x.\n",
    "The `fit()` method identifies the missing values and computes the mean of such feature a missing value is present.\n",
    "'''\n",
    "imputa.fit(X[:, 1:3]) # identifies the missing values and computes the mean of such feature where a missing value is present.\n",
    "\n",
    "# Replace the missing value using transform method\n",
    "X[:, 1:3] = imputa.transform(X[:, 1:3])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Categorical Data\n",
    "During encoding, we transform text data into numeric data.<br>\n",
    "Country and Purchased column data are categorical data.<br>\n",
    "It is not possible to compute correlation between feature and dependent variables when mathematical equations in machine learning takes only numeric inputs.<br>\n",
    "Therefore, need to convert string entries in dataset into numeric values.\n",
    "\n",
    "For our dataset, encode:\n",
    "- France into 0\n",
    "- Spain into 1\n",
    "- Germany into 2\n",
    "\n",
    "But machine learning model will interpret this in numerical order between 0 for France, 1 for Spain, and 2 for Germany.<br>\n",
    "But this may not always be the case.<br><br>\n",
    "To prevent this misinterpretation, we make use of <b>one-hot encoding<br>\n",
    "One-hot encoding converts categorical Country column into three columns.<br>\n",
    "It creates a unique binary vector for each country such that there is no numerical order between the country categories.\n",
    "\n",
    "In cases where there are high cardinality categorical variables, one-hot encoding may generate many new features. <br>\n",
    "This is not a practical solution.<br>\n",
    "Proposed solution is to use <b>Target Encoding</b> or <b>Leave-one-out Encoding</b> to handle these high cardinality categorical features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Import classes\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder= 'passthrough')\n",
    "X = np.array(ct.fit_transform(X))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- The Country column has been transformed into 3 columns with each row representing only one encoded column\n",
    "- France is encoded into a vector [1.0 0.0 0.0]\n",
    "- Spain is encoded into a vector [0.0 0.0 1.0]\n",
    "- Germany is encoded into a vector [0.0 1.0 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Encode dependent variable y\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder() \n",
    "y = le.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- Use LabelEncoder for dependent variable y because there are only two values for y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into Training and Test Sets\n",
    "Split dataset into a training set and a test set.<br>\n",
    "The training set is the fraction of a dataset used to implement the model.<br>\n",
    "The test set is the fraction of a dataset used to evaluate the performance of the model.<br>\n",
    "The test set is assumed to be unknown during the process of the model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train\n",
      " [[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n",
      "\n",
      "X_test\n",
      " [[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n",
      "\n",
      "y_train\n",
      " [0 1 0 0 1 1 0 1]\n",
      "\n",
      "y_test\n",
      " [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Print\n",
    "print(\"\\nX_train\\n\", X_train)\n",
    "print(\"\\nX_test\\n\", X_test)\n",
    "print(\"\\ny_train\\n\", y_train)\n",
    "print(\"\\ny_test\\n\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- Our features set was divided into eight observations for the X_train and 2 for the X_test, which correspond (since we set our seed, random = 1) to the same splitting of the dependent variable y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale Features\n",
    "Datasets may not have features of the same scale.<br>\n",
    "Some features often have tremendous values, and others have small values.<br>\n",
    "Suppose we implement our machine learning model on such datasets. <br>\n",
    "In that case, features with tremendous values dominate those with small values, and the machine learning model treats those with small values as if they donâ€™t exist (their influence on the data is not be accounted for). <br>\n",
    "To ensure this is not the case, we need to scale our features on the same range, i.e., within the interval of -3 and 3.\n",
    "\n",
    "We need to scale the Age and Salary columns of X_train and X_test into this interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply the feature scaling on the features other than dummy variables.\n",
    "X_train[:, 3:] = scaler.fit_transform(X_train[:, 3:])\n",
    "X_test[:, 3:]  = scaler.fit_transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train\n",
      " [[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n",
      "\n",
      "X_test\n",
      " [[0.0 1.0 0.0 -1.0 -1.0]\n",
      " [1.0 0.0 0.0 1.0 1.0]]\n"
     ]
    }
   ],
   "source": [
    "# Print\n",
    "print(\"\\nX_train\\n\", X_train)\n",
    "print(\"\\nX_test\\n\", X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- Only Age and Salary columns and not the dummy variables are scaled. \n",
    "- This is because scaling the dummy variables may interfere with their intended interpretation even though they fall within the required range.\n",
    "- The Age and Salary features are now scaled on the same range, i.e., within the interval of -3 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"MLSD-02-DataPreprocessing-B.ipynb\" target=\"_self\">Data Preprocessing B</a> | <a href=\"./\">Content Page</a> | <a href=\"MLSD-02-DataPreprocessing-D.ipynb\">Data Preprocessing D | <a href=\"MLSD-02-DataPreprocessing-Ex-1.ipynb\">Data Preprocessing Exercise</a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"MLSD-04-FeatureEngineering-Ex-3.ipynb\" target=\"_self\">Feature Engineering Exercise 3</a> | <a href=\"./\">Content Page</a> | <a href=\"MLSD-05-FeatureSelection-B.ipynb\">Feature Selection B | <a href=\"MLSD-05-FeatureSelection-Ex-1.ipynb\">Feature Selection Exercise 1</a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>FEATURE SELECTION A</center>\n",
    "\n",
    "<center><b>Copyright &copy 2023 by DR DANNY POO</b><br> e:dannypoo@nus.edu.sg<br> w:drdannypoo.com</center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition\n",
    "Feature selection is the process where you automatically or manually select the features that contribute the most to your prediction variable or output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance\n",
    "Having irrelevant features in your data can decrease the accuracy of the machine learning models.\n",
    "\n",
    "The top reasons to use feature selection are:\n",
    "- It enables the machine learning algorithm to train faster.\n",
    "- It reduces the complexity of a model and makes it easier to interpret.\n",
    "- It improves the accuracy of a model if the right subset is chosen.\n",
    "- It reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Methods\n",
    "**Univariate Selection**\n",
    "- Statistical tests can help to select independent features that have the strongest relationship with the target feature in your dataset e.g. chi-square test.\n",
    "- The Scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\n",
    "- <b>Chi-square Test</b>\n",
    "- <b>Logit (Logistic Regression model)</b>\n",
    "\n",
    "**Feature Importance**\n",
    "- Feature importance gives you a score for each feature of your data. \n",
    "- The higher the score, the more important or relevant that feature is to your target feature.\n",
    "- Feature importance is an inbuilt class that comes with tree-based classifiers such as: Random Forest Classifiers and <b>Extra Tree Classifiers</b>\n",
    "\n",
    "**Correlation**\n",
    "- Correlation shows how the features are related to each other or the target feature.\n",
    "- Correlation can be positive (an increase in one value of the feature increases the value of the target variable) or negative (an increase in one value of the feature decreases the value of the target variable).\n",
    "- <b>Pearson Correlation</b>\n",
    "- <b>Correlation Matrix Heatmap</b>\n",
    "\n",
    "**Iterative Search**\n",
    "- <b>Forward stepwise selection</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load iris data\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and target\n",
    "X = iris_dataset.data\n",
    "y = iris_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to categorical data by converting data to integers\n",
    "X = X.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two features with highest chi-squared statistics are selected\n",
    "chi2_features = SelectKBest(chi2, k = 2)\n",
    "X_kbest_features = chi2_features.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced features\n",
    "print('Original feature number:', X.shape[1])\n",
    "print('Reduced  feature number:', X_kbest_features.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- Selected two important independent features out of the original 4 that have the strongest relationship with the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kbest_features[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris data\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and target\n",
    "X = iris_dataset.data\n",
    "y = iris_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to categorical data by converting data to integers\n",
    "X = X.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "extra_tree_forest = ExtraTreesClassifier(n_estimators = 5,\n",
    "                                        criterion ='entropy', max_features = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "extra_tree_forest.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the importance of each feature\n",
    "feature_importance = extra_tree_forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the individual importances\n",
    "feature_importance_normalized = np.std([tree.feature_importances_ for tree in \n",
    "                                        extra_tree_forest.estimators_],\n",
    "                                        axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting a Bar Graph to compare the models\n",
    "plt.bar(iris_dataset.feature_names, feature_importance_normalized)\n",
    "plt.xlabel('Feature Labels')\n",
    "plt.ylabel('Feature Importances')\n",
    "plt.title('Comparison of different Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- The most important features are petal length (cm) and  petal width (cm).\n",
    "- The least important feature is sepal width (cms). \n",
    "- This means you can use the most important features to train your model and get best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataframe = pd.read_csv('./data/housePrices/train.csv')\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe[['OverallQual','TotalBsmtSF','GarageArea','GarageCars','1stFlrSF','YearBuilt','YearRemodAdd','MasVnrArea','FullBath','GrLivArea','TotRmsAbvGrd','MoSold','YrSold','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12);\n",
    "# save heatmap as .png file\n",
    "# dpi - sets the resolution of the saved image in dots/inches\n",
    "# bbox_inches - when set to 'tight' - does not allow the labels to be cropped\n",
    "plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- The correlation coefficient ranges from -1 to 1. \n",
    "- If the value is close to 1, it means that there is a strong positive correlation between the two features. \n",
    "- When it is close to -1, the features have a strong negative correlation.\n",
    "- Features in the dataset that are correlated to each other suggests they convey the same information. It is recommended to remove one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"MLSD-04-FeatureEngineering-Ex-3.ipynb\" target=\"_self\">Feature Engineering Exercise 3</a> | <a href=\"./\">Content Page</a> | <a href=\"MLSD-05-FeatureSelection-B.ipynb\">Feature Selection B | <a href=\"MLSD-05-FeatureSelection-Ex-1.ipynb\">Feature Selection Exercise 1</a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
